# Retail Management System using Apache Kafka Streaming Service with Gilhari
This is a case-based example of how to use the Gilhari microservice framework to transfer json data from an Apache Kafka streaming server to a PostgreSQL database.
* This project simulates a retail business in which 3 types of data - Employee data (as hires), Sales data (as sales transactions), and Inventory data (as shipment transactions) are being continuously generated.
* 3 tables exist in the database to record such information:
    1. Employee
    2. Sales
    3. Inventory
* Each type is its own topic, and each topic's data stream is being generated by its own producer python script - `src/employees_producer.py`, `src/sales_producer.py`, `src/inventory_producer.py`.
* The data in the topics are received by a consumer in the client python script `src/consumer_client.py`.
* The client makes requests to the Gilhari REST API (GET, POST, PATCH requests) to make changes to the tables in the database. 
* The producers and consumer scripts run independent of one another.
* Notice that using the Gilhari microservice framework in the consumer program eliminates the need to write any SQL code for storing JSON data in a relational database.\
Gilhari does not require any database native JSON data type. \
Also, the database-agnostic framework of Gilhari makes it very easy to switch the backend relational database (e.g., from Postgres to MySQL) without having to write any code.

The steps to configure Gilhari, the PostgreSQL database, Kafka, the project environment and the steps to run the project are given below.

# Setting up Gilhari microservice
Proceed to `/java_src/README.md` and follow the instructions there

# Setting up the PostgreSQL database
With a user `postgres` and password `psql`, create a new database called `gilharikafkadb`

# Setting up project environment
Follow these steps to set up your dev/testing environment

## Step 1: Install Python 3.10
Install python 3.10 from official sources. Ensure you are using python 3.10 by running the command `python --version`

## Step 2: Cloning repository and setting up virtual environment
Clone this repository and navigate to the root of your local repository in a terminal. Then, run the command `python -m venv gilhari-kafka-env`

Activate the environment by running the command:

`gilhari-kafka-env/Scripts/activate.bat` (Windows command prompt)

`gilhari-kafka-env/Scripts/activate.ps1` (Windows powershell)

`source gilhari-kafka-env/bin/activate` (MacOS/Linux)

## Step 3: Install requirements from requirements.txt
run the command `pip install -r requirements.txt` after activating the virtual environment

verify installation by running the command `pip list`

# Setting up Kafka data streaming server
Navigate to your kafka directory (eg, home/usr/kafka_2.13-3.7.0) and run the following commands in separate terminals

## Start zookeeper server (default port 2181)
run the command `bin/zookeeper-server-start.sh config/zookeeper.properties`

## Start kafka server (default port 9092)
run the command `bin/kafka-server-start.sh config/server.properties`

## Stopping the servers
run the commands

`bin/kafka-server-stop.sh`

`bin/zookeeper-server-stop.sh`

# Running the project
Follow the above steps to set up both servers and the docker container. Then in a new terminal located in the root directory of the project, proceed to the following:

## Generate data using a producer application
run the commands `python src/inventory_producer.py`, `python src/employees_producer.py`, `python src/sales_producer.py`

**NOTE**: The sales transactions require there to be inventory records as in any retail business.\
There exists no explicit code to populate the inventory.\
You must run just the inventory production driver and consumer to first populate the inventory.\
Then you may run all 3 producers with the consume simultaneously. 

## Retrieving json data using the consumer
run the command `python src/consumer_client.py`
