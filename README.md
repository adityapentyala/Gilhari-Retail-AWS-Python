# Retail Management System using Apache Kafka Streaming Service with Gilhari

## About Gilhari
This project uses Gilhari microservice framework to exchange JSON data with relational databases. Gilhari is a product of Software Tree, LLC. GilhariTM is a microservice framework to provide persistence for JSON objects in relational databases. Available in a Docker image, it is configurable as per an app-specific object and relational models. Gilhari exposes a REST (REpresentational State Transfer) interface to provide APIs (POST, GET, PUT, DELETEâ€¦) for CRUD (Create, Retrieve, Update, and Delete) operations on the app-specific JSON objects. You may get more information about Gilhari and its SDK at https://www.softwaretree.com.

## About the Project
This is a case-based example of how to use the Gilhari microservice framework to transfer json data from an Apache Kafka streaming server to a PostgreSQL/MySQL/SQLite database.
* This project simulates a retail business in which 3 types of data - Employee data (as hires), Sales data (as sales transactions), and Inventory data (as shipment transactions) are being continuously generated.
* 3 tables exist in the database to record such information:
    1. Employee
    2. Sales
    3. Inventory
* Each type is its own topic, and each topic's data stream is being generated by its own producer python script - `src/employees_producer.py`, `src/sales_producer.py`, `src/inventory_producer.py`.
* The data in the topics are received by a consumer in the client python script `src/consumer_client.py`.
* The client makes requests to the Gilhari REST API (GET, POST, PATCH requests) to make changes to the tables in the database. 
* The producers and consumer scripts run independent of one another.
* Notice that using the Gilhari microservice framework in the consumer program eliminates the need to write any SQL code for storing JSON data in a relational database.\
Gilhari does not require any database native JSON data type. \
Also, the database-agnostic framework of Gilhari makes it very easy to switch the backend relational database (e.g., from Postgres to MySQL/SQLite) without having to write any code.

The steps to configure Gilhari, the database(s), Kafka, the project environment and the steps to run the project are given below.

# Setting up Gilhari microservice
Proceed to `/Gilhari9/README.md` and follow the instructions there

# Setting up the database
The `.jdx` ORM file in `Gilhari9/config/` contains the mapping commands for each of these databases. To change, simply comment out the ones that are not in use, and change the path to the JDBC driver in `Gilhari9/gilhari_service.config` to the respective database's JDBC driver `.jar` file. 
## PostgreSQL
With a user `postgres` and password `psql`, create a new database called `gilharikafkadb`. Run the server on localhost at port 5432.

## MySQL
With a user `mysqluser` and password `mysqlpassword`, create a new database called `gilharikafkadb`. Run the server on localhost at port 3306.

*NOTE: You may change the authentication details and port numbers for each of the above databases. Make sure you change the same in the `.jdx` ORM file.*

## SQLite
No authentication details are required. Simply make sure that the docker container is mapped to a local volume so that changes made by gilhari are reflected in the local volume and persist even after the container is stopped. Details on the CLI argument to be passed to ensure this are given in the `README.md` file in `Gilhari9/`.

# Setting up project environment
Follow these steps to set up your dev/testing environment

## Step 1: Install Python 3.10
Install python 3.10 from official sources. Ensure you are using python 3.10 by running the command `python --version`

## Step 2: Cloning repository and setting up virtual environment
Clone this repository and navigate to the root of your local repository in a terminal. Then, run the command `python -m venv gilhari-kafka-env`

Activate the environment by running the command:

`gilhari-kafka-env/Scripts/activate.bat` (Windows command prompt)

`gilhari-kafka-env/Scripts/activate.ps1` (Windows powershell)

`source gilhari-kafka-env/bin/activate` (MacOS/Linux)

## Step 3: Install requirements from requirements.txt
run the command `pip install -r requirements.txt` after activating the virtual environment

verify installation by running the command `pip list`

# Setting up Kafka data streaming server
Navigate to your kafka directory (eg, home/usr/kafka_2.13-3.7.0) and run the following commands in separate terminals

## Start zookeeper server (default port 2181)
run the command `bin/zookeeper-server-start.sh config/zookeeper.properties`

## Start kafka server (default port 9092)
run the command `bin/kafka-server-start.sh config/server.properties`

## Stopping the servers
run the commands

`bin/kafka-server-stop.sh`

`bin/zookeeper-server-stop.sh`

# Running the project
Follow the above steps to set up both servers and the docker container. Then in a new terminal located in the root directory of the project, proceed to the following:

## Generate data using a producer application
run the commands `python src/inventory_producer.py`, `python src/employees_producer.py`, `python src/sales_producer.py`

**NOTE**: The sales transactions require there to be inventory records as in any retail business.\
There exists no explicit code to populate the inventory.\
You must run just the inventory production driver and consumer to first populate the inventory.\
Then you may run all 3 producers with the consumer simultaneously. 

## Retrieving json data using the consumer
run the command `python src/consumer_client.py`
